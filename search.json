[{"title":"阅读笔记01-Preemptive Low Latency Datacenter Scheduling via Lightweight Virtualization","url":"/20190325/阅读笔记01-atc2017-bigc/","content":"\n# 基本信息\n会议：[the 2017 USENIX Annual Technical Conference (USENIX ATC '17)](https://www.usenix.org/conference/atc17/technical-sessions/presentation/chen-wei)       \n文章原文：[paper](https://www.usenix.org/system/files/conference/atc17/atc17-chen_wei.pdf)       \n会议报告：[ppt](https://www.usenix.org/sites/default/files/conference/protected-files/atc_slides_chen_wei_0.pdf)\n\n# 动机与问题   \n为了提高集群资源利用率，数据中心趋向于部署异构负载。但是不同负载特性和需求不同，一方面，对延迟敏感的任务需要尽可能快地被调度到目的节点；另一方面，长时间运行的任务应该在有资源空闲的时候占用集群空闲资源来提高资源利用率。问题的关键在于如何在保证短作业性能的同时，确保集群的资源利用率最大化。\n# 解决方案\n文章提出了一种基于容器的任务抢占方式。由于容器具有可以动态调整其运行资源的特性，通过使用容器作为任务的执行单元可以实现对任务资源的动态调整。\n基于Hadoop YARN的架构如图所示。   \n![bigc-arch](阅读笔记01-atc2017-bigc/bigc-arch.png)\n\n# 分析\n\n\n\n\n\n","tags":["Docker","Resource Scheduling","Hadoop YARN"],"categories":["阅读笔记"]},{"title":"使用ss或者ssr实现Linux系统全局FQ","url":"/20180615/使用ss或者ssr实现Linux全局FQ/","content":"\n# 使用方法：\n\n下载https://github.com/yujinyu/installSStproxyEasily.git\n\n1.在conf.py中选择使用代理方式是ss还是ssr，并配置账号、密码以及其他参数；\n\n2.运行install_ss-redir.py安装软件并进行环境配置；\n\n3.安装配置完成后，运行命令\"ss-tproxy start\"开始使用。\n\n\n# 参考资料：\n[1]. https://www.zfl9.com/ss-redir.html\n\n[2]. https://github.com/shadowsocksr-backup/shadowsocksr-libev\n\n[3]. https://github.com/shadowsocks/shadowsocks-libev.git\n\n[4]. https://github.com/zfl9/ss-tproxy.git\n","tags":["ShadowsocksR","Shadowsocks"],"categories":["搞机记录"]},{"title":"基于Kubernetes部署kong 0.13.0","url":"/20180411/基于Kubernetes部署kong 0.13.0/","content":"\n# （一） 一些准备工作\n\n#### 1. Kubernetes部署完成并且支持DNS解析；\n#### 2. 准备运行数据库（Postgres或者Cassandra）容器的必要的后端存储，这里我使用的heketi-glusterfs;\n#### 3. 特别说明，本文kong应用部署在default namespace中\n# （二）Kong部署过程\n## 1. 创建StorageClass，为运行数据库Postgres或者Cassandra做准备；\n\n``` yaml?linenums\n# cat glusterfs-storageclass.yaml    \napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: glusterfs-sc\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\nprovisioner: kubernetes.io/glusterfs\nparameters:\n  resturl: \"http://heketi-sever:port\"\n  restuser: \"username\"\n  restuserkey: \"password\"\n```\n使用glusterfs-storageclass.yaml在Kubernetes中创建StorageClass glusterfs-sc。\n``` bash\nkubectl create -f glusterfs-storageclass.yaml \n```\n## 2. 运行并初始化数据库\n官方给出了支持的两种数据库：Postgres和Cassandra，在此我选择了Cassandra。\n###  1. 运行Cassandra数据库\nCassandra服务以及StatefulSet的描述文件如下：\n\n``` YAML?linenums\n# cat cassandra.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: cassandra\n  name: cassandra\nspec:\n  clusterIP: None\n  ports:\n    - port: 9042\n  selector:\n    app: cassandra\n---\napiVersion: \"apps/v1beta1\"\nkind: StatefulSet\nmetadata:\n  name: cassandra\nspec:\n  serviceName: cassandra\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: cassandra\n    spec:\n      containers:\n      - name: cassandra\n        # image: yujinyu/cassandra:v12\n        image: cassandra\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 7000\n          name: intra-node\n        - containerPort: 7001\n          name: tls-intra-node\n        - containerPort: 7199\n          name: jmx\n        - containerPort: 9042\n          name: cql\n        env:\n          - name: MAX_HEAP_SIZE\n            value: 512M\n          - name: HEAP_NEWSIZE\n            value: 100M\n          - name: CASSANDRA_SEEDS\n            # 根据部署应用所在的namespace修改以下的值\n            value: \"cassandra-0.cassandra.default.svc.cluster.local\"\n          - name: CASSANDRA_CLUSTER_NAME\n            value: \"K8Demo\"\n          - name: CASSANDRA_DC\n            value: \"DC1-K8Demo\"\n          - name: CASSANDRA_RACK\n            value: \"Rack1-K8Demo\"\n          - name: CASSANDRA_AUTO_BOOTSTRAP\n            value: \"false\"\n          - name: POD_IP\n            valueFrom:\n              fieldRef:\n                fieldPath: status.podIP\n        volumeMounts:\n        - name: cassandra-data\n          mountPath: /cassandra_data\n  volumeClaimTemplates:\n 1. metadata:\n      name: cassandra-data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      storageClassName: glusterfs-sc\n      resources:\n        requests:\n          storage: 10Gi\n```\n使用以上描述文件部署Cassandra服务以及StatefulSet，\n``` bash\nkubectl create -f cassandra.yaml\n```\n###  2. 准备Cassandra数据库\n\n``` yaml?linenums\n# cat kong_migration_cassandra.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: kong-migration\nspec:\n  template:\n    metadata:\n      name: kong-migration\n    spec:\n      containers:\n      - name: kong-migration\n        image: kong:0.13.0-centos\n        env:\n          - name: KONG_NGINX_DAEMON\n            value: 'off'\n          - name: KONG_DATABASE\n            value: cassandra\n          - name: KONG_CASSANDRA_CONTACT_POINTS\n            value: cassandra\n          - name: KONG_CASSANDRA_KEYSPACE\n            value: kong\n        command: [ \"/bin/sh\", \"-c\", \"kong migrations up\" ]\n      restartPolicy: Never\n```\n运行kong-migration job准备并更新数据库，\n\n``` bash\nkubectl create -f kong_migration_cassandra.yaml\n```\n一旦job完成即在使用命令“kubectl get job kong-migration\"显示内容中success下面显示为“1”，通过以下命令移除该job,\n\n``` bash\nkubectl delete -f kong_migration_cassandra.yaml\n```\n###  3.运行kong\nkong服务以及Deploy描述文件如下，\n``` yaml?linenums\n# cat kong_cassandra.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: kong-proxy\nspec:\n  type: LoadBalancer\n  loadBalancerSourceRanges:\n  - 0.0.0.0/0\n  ports:\n  - name: kong-proxy\n    port: 8000\n    targetPort: 8000\n    protocol: TCP\n  selector:\n    app: kong\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kong-proxy-ssl\nspec:\n  type: LoadBalancer\n  loadBalancerSourceRanges:\n  - 0.0.0.0/0\n  ports:\n  - name: kong-proxy-ssl\n    port: 8443\n    targetPort: 8443\n    protocol: TCP\n  selector:\n    app: kong\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kong-admin\nspec:\n  type: LoadBalancer\n  loadBalancerSourceRanges:\n  - 0.0.0.0/0\n  ports:\n  - name: kong-admin\n    port: 8001\n    targetPort: 8001\n    protocol: TCP\n  selector:\n    app: kong\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kong-admin-ssl\nspec:\n  type: LoadBalancer\n  loadBalancerSourceRanges:\n  - 0.0.0.0/0\n  ports:\n  - name: kong-admin-ssl\n    port: 8444\n    targetPort: 8444\n    protocol: TCP\n  selector:\n    app: kong\n\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: kong-rc\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        name: kong-rc\n        app: kong\n    spec:\n      containers:\n      - name: kong\n        image: kong:0.13.0-centos\n        env:\n          - name: KONG_ADMIN_LISTEN\n            value: \"0.0.0.0:8001, 0.0.0.0:8444 ssl\"\n          - name: KONG_DATABASE\n            value: cassandra\n          - name: KONG_CASSANDRA_CONTACT_POINTS\n            value: cassandra\n          - name: KONG_CASSANDRA_KEYSPACE\n            value: kong\n          - name: KONG_CASSANDRA_REPL_FACTOR\n            value: \"2\"\n          - name: KONG_PROXY_ACCESS_LOG\n            value: \"/dev/stdout\"\n          - name: KONG_ADMIN_ACCESS_LOG\n            value: \"/dev/stdout\"\n          - name: KONG_PROXY_ERROR_LOG\n            value: \"/dev/stderr\"\n          - name: KONG_ADMIN_ERROR_LOG\n            value: \"/dev/stderr\"\n        ports:\n        - name: admin\n          containerPort: 8001\n          protocol: TCP\n        - name: proxy\n          containerPort: 8000\n          protocol: TCP\n        - name: proxy-ssl\n          containerPort: 8443\n          protocol: TCP\n        - name: admin-ssl\n          containerPort: 8444\n          protocol: TCP\n```\n使用描述文件部署kong，\n\n``` bash\nkubectl create -f kong_cassandra.yaml\n```\n\n###  4.【可选，只适用于Cassandra】保证数据库的高可靠性\n为保证Cassandra的高可靠性扩展Cassandra StatefulSet的副本至3个，\n\n``` bash\nkubectl scale --replicas=3 statefulset/cassandra\n```\n\n# （三） 验证并使用kong\n## 1. 直接访问kong\n\n``` bash\ncurl http://${kubernetes_master_ip}:$(kong_admin_nodeport} | python -m json.tool\n```\n如成功部署kong，运行以上命令不出错并有数据显示。\n## 2. 运行Nginx实例测试\n### 1. 部署测试应用Nginx\n\n``` yaml?linenums\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-nginx\n  labels:\n    run: my-nginx\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n  selector:\n    run: my-nginx\n```\n### 2. 在kong中添加api\n\n``` bash\ncurl -i -X POST --url http://${kubernetes_master_ip}:${kong_admin_nodeport}/apis/ --data 'name=nginx-example-api'   --data 'hosts=nginx-example.com'   --data 'upstream_url=http://my-nginx'\n```\n说明：如果部署Nginx应用和Kong不在同一namespace，则需修改命令中“\nupstream_url”的值，加入kong在defaul而Nginx在test命名空间，那么upstream_url则应修改为“upstream_url=http://my_nginx.test.svc.cluster.local\"（cluster-domain默认为cluster.local）。\n\n### 3. 使用api访问后端Nginx服务\n\n``` bash\ncurl -i -X GET  --url http://${kubernetes_master_ip}:$(kong_proxy_nodeport}/  --header 'Host: nginx-example.com'\n```\n结果如图所示，\n![enter description here](基于Kubernetes部署kong 0.13.0/clipboard.png)\n\n# （四） 到此已成功运行kong。\n\n\n----------\n\n参考资料：   \n[1]. https://github.com/Kong/kong-dist-kubernetes      \n[2]. https://getkong.org/install/kubernetes/     \n[3]. https://kubernetes.io/docs/concepts/storage/storage-classes/     \n[4]. https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/     \n[5]. https://github.com/IBM/Scalable-Cassandra-deployment-on-Kubernetes     \n[6]. https://getkong.org/docs/0.13.x/getting-started/configuring-a-service/     \n","tags":["Kubernetes","API Gateway","Kong"],"categories":["搞机记录"]},{"title":"使用consul构建跨主机的docker overlay网络","url":"/20171020/使用consul构建docker overlay网络/","content":"\n#### 一、提前说明\nIp: 192.168.3.xx\n\n节点主机: de69,de75,de79\nconsul server主机: de82\n\n配置环境之前请关闭防火墙\nUbuntu/Debian:\n\n```\nufw disable && setenforce 0\n```\nCentos/Fedora:\n\n```\nsystemctl disable firewalld && systemctl stop firewalld && setenforce 0\n```\n\n#### 二、配置consul服务器\n在de82主机上运行 \n\n```\ndocker run -d --restart=always --name=consul-server\\\n    -p \"8500:8500\" \\\n    -h \"consul\" \\\n    progrium/consul -server -bootstrap\n```\n将其作为consul服务器主机。\n\n#### 三、配置负载节点\n##### 修改de69、de75、de79节点主机上的/lib/systemd/system/docker.service，添加参数\n\n -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2376  --cluster-store=consul://${serverip}:8500 --cluster-advertise ${hostip}:2376\n\nCentos/Fedora系统\n\n```\nExecStart=/usr/bin/dockerd -H  unix://var/run/docker.sock -H tcp://0.0.0.0:2376 --cluster-store=consul://192.168.3.82:8500 --cluster-advertise 192.168.3.75:2376 \n```\n\nUbuntu/Debian系统\n\n```\nExecStart=/usr/bin/dockerd -H fd:// -H  unix://var/run/docker.sock -H tcp://0.0.0.0:2376 --cluster-store=consul://192.168.3.82:8500 --cluster-advertise=192.168.3.69:2376 \n```\n\n##### 重启docker daemon\n\n```\nsystemctl daemon-reload && systemctl restart docker\n```\n\n#### 四、创建overlay网络\n在其中一个节点主机上新建一个overlay网络，\n\n```\ndocker network create -d overlay ${networkname}\n```\n这里我在de69上创建了名称为net1的网络：\n\n![de69nwls](使用consul构建跨主机的docker overlay网络/de69nwls.PNG)\n\n\n其他节点主机上运行docker network ls 也可以看到并且使用该overlay网络。\n![de75nwls](使用consul构建跨主机的docker overlay网络/de75nwls.PNG)\n\n#### 五、使用overlay网络创建跨主机通信的容器\n\n```\n docker run -itd --name ${container_name} --network net1 busybox\n```\n在de69创建容器bbox5，在de75 bbpx1 bbox4,在de79 bbox2 bbox3。\n\n在de69中ping测试：\n\n![de69ping](使用consul构建跨主机的docker overlay网络/de69ping.PNG)\n\n在de75中ping测试：\n\n![de75ping](使用consul构建跨主机的docker overlay网络/de75ping.PNG)\n","tags":["Docker","Overlay Network"],"categories":["搞机记录"]},{"title":"lock_stat使用记录","url":"/20170926/lock_stat使用记录/","content":"\n\n### 官方文档\n\nFrom $(path}/linux-x.x.x/Documentation/locking/lockstat.txt\n\n#### LOCK STATISTICS\n\n- WHAT\n\nAs the name suggests, it provides statistics on locks.\n\n- WHY\n\nBecause things like lock contention can severely impact performance.\n\n- HOW\n\nLockdep already has hooks in the lock functions and maps lock instances to\nlock classes. We build on that (see Documentation/locking/lockdep-design.txt).\nThe graph below shows the relation between the lock functions and the various\nhooks therein.\n\n        __acquire\n            |\n           lock _____\n            |        \\\n            |    __contended\n            |         |\n            |       <wait>\n            | _______/\n            |/\n            |\n       __acquired\n            |\n            .\n          <hold>\n            .\n            |\n       __release\n            |\n         unlock\n\nlock, unlock\t- the regular lock functions\n\n__*\t\t- the hooks\n\n<> \t\t- states\n\nWith these hooks we provide the following statistics:\n\n>  con-bounces       - number of lock contention that involved x-cpu data    \n>  contentions       - number of lock acquisitions that had to wait    \n>  wait time min     - shortest (non-0) time we ever had to wait for a lock     \n>  wait time max     - longest time we ever had to wait for a lock            \n>  wait time total   - total time we spend waiting on this lock      \n>  wait time avg     - average time spent waiting on this lock       \n>  acq-bounces       - number of lock acquisitions that involved x-cpu data     \n>  acquisitions      - number of times we took the lock     \n>  hold time min     - shortest (non-0) time we ever held the lock        \n>  hold time max     - longest time we ever held the lock        \n>  hold time total   - total time this lock was held       \n>  hold time avg     - average time this lock was held         \n\nThese numbers are gathered per lock class, per read/write state (when\napplicable).\n\nIt also tracks 4 contention points per class. A contention point is a call site that had to wait on lock acquisition.\n\n - CONFIGURATION\n\nLock statistics are enabled via CONFIG_LOCK_STAT.\n\n - USAGE\n\nEnable collection of statistics:\n\n`# echo 1 >/proc/sys/kernel/lock_stat`   \n \nDisable collection of statistics:\n\n`# echo 0 >/proc/sys/kernel/lock_stat`\n\nLook at the current lock statistics:\n\n( line numbers not part of actual output, done for clarity in the explanation below )\n\n`# less /proc/lock_stat`\n\n![less](lock_stat使用记录/lock1.PNG)\n\nThis excerpt shows the first two lock class statistics. Line 01 shows the output version - each time the format changes this will be updated. Line 02-04 show the header with column descriptions. Lines 05-18 and 20-31 show the actual statistics. These statistics come in two parts; the actual stats separated by a short separator (line 08, 13) from the contention points. \nLines 09-12 show the first 4 recorded contention points (the code which tries to get the lock) and lines 14-17 show the first 4 recorded contended points (the lock holder). It is possible that the max \ncon-bounces point is missing in the statistics.\n\nThe first lock (05-18) is a read/write lock, and shows two lines above the short separator. The contention points don't match the column descriptors, they have two: contentions and [<IP>] symbol. The second set of contention\npoints are the points we're contending with.\n\nThe integer part of the time values is in us.\n\nDealing with nested locks, subclasses may appear:             \n        \n![lock](lock_stat使用记录/lock2.PNG)\n\nLine 48 shows statistics for the second subclass (/1) of &rq->lock class (subclass starts from 0), since in this case, as line 50 suggests, double_rq_lock actually acquires a nested lock of two spinlocks.  \nView the top contending locks:                          \n`# grep : /proc/lock_stat | head`  \n\n![head](lock_stat使用记录/lock3.PNG)\n\nClear the statistics:   \n`# echo 0 > /proc/lock_stat`  \n### 对某一进程或者应用进行lock stat  \n##### 关闭lock_stat  \n`echo 0 > /proc/sys/kernel/lock_stat` \n##### 清空之前的统计数据  \n`echo 0 > /proc/lock_stat`  \n##### 同时运行应用（或进程）和启用lock stat  \n`echo 1 > /proc/sys/kernel/lock_stat`  \n##### 应用（或进程）运行结束关闭lock stat  \n`echo 0 > /proc/sys/kernel/lock_stat`  \n##### 此时/proc/lock_stat中的数据就是应用（或进程）运行过程中的lock数据分析。  \n\n首次尝试使用lock_stat，Mark一下。  \n\n","tags":["lock_stat"],"categories":["搞机记录"]},{"title":"使用Kubernetes1.4.6源码搭建容器集群","url":"/20161122/使用Kubernetes1.4.6源码搭建容器集群/","content":"\n### 一、相关准备工作\n\n#### 1.1、准备工作\n\n - 准备至少两台已安装好CentOS7.2操作系统的物理机或者虚拟机（本文配置时使用的是三台KVM虚拟机）；\n - 设置hostname命令：`hostnamectl set-hostname k8s-mst`\n\n>| 角色   | ip           | hostname |\n>|  ------- | ------- |  ------- |\n>| Master | 192.168.3.87 | k8s-mst   |\n>| Node   | 192.168.3.88 | k8s-nod1 |\n>| Node   | 192.168.3.89 |k8s-nod2  |\n\n\n- 为了避免和Docker的iptables产生冲突，需要关闭Node节点上的防火墙\n`systemctl stop firewalld`\n`systemctl disable firewalld`\n\n- 为了让各个节点的时间保持一致，需要为所有节点安装NTP\n`yum -y install ntp`\n`systemctl start ntpd`\n`systemctl enable ntpd`\n\n#### 1.2、编译Kubernetes源码(建议在Node节点上）\n\n####  1.2.1、安装docker-engine\n#### [官方指导资料](https://docs.docker.com/engine/installation/linux/centos/)\n#### 使用以下方法可以安装较新版本\n\n- 添加yum库\n\n```\nsudo tee /etc/yum.repos.d/docker.repo <<-'EOF'\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/experimental/centos/7/\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF\n```\n\n - 安装Docker-engine\n `sudo yum install -y docker-engine`\n\n\n - 运行Docker Daemon\n `sudo systemctl start docker`\n - 可以使用`sudo systemctl status docker`查看docker Daemon的运行状态\n\n#### 1.2.2、安装Golang\n`sudo yum install -y golang`\n\n#### 1.2.3、下载Kubernetes\n- 从Kubernetes的github下载源码\ngit clone https://github.com/kubernetes/kubernetes.git\n- 或者直接下载相应版本的release包https://codeload.github.com/kubernetes/kubernetes/tar.gz/v1.4.6\n- 如果使用git clone，下载完成后进入kubernetes文件夹，使用命令`git checkout v1.4.6`，下载release包则解压`tar -xvf kubernetes-1.4.6.tar.gz`进入kubernetes文件夹；\n\n#### 1.2.4、编译Kubernetes\n - 修改hosts\n由于在Kubernetes编译过程中需要pull谷歌容器库（gcr）中的相关镜像，故需要修改hosts进行翻墙，hosts文件参考：https://github.com/racaljk/hosts\n\n - 修改运行平台配置参数\n根据自己的运行平台（linux/amd64)修改hack/lib/golang.sh，把KUBE_SERVER_PLATFORMS，KUBE_CLIENT_PLATFORMS和KUBE_TEST_PLATFORMS中除linux/amd64以外的其他平台注释掉，以此来减少编译所用时间\n\n\n - 编译源码\n在Kubernetes根目录下运行命令`make release-skip-tests`，编译耗时相对较长\n\n - 编译成功之后，可执行文件在文件夹“_output”中\n\n### 二、Master配置工作\n\n#### 2.1、安装ectd并修改配置文件\n#### 2.1.1、安装必要软件etcd\n`yum -y install etcd`\n#### 2.1.2、  修改etcd的配置文件/etc/etcd/etcd.conf\n\n```\nETCD_NAME=default\nETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"\nETCD_LISTEN_CLIENT_URLS=\"http://0.0.0.0:2379\"\n```\n#### 2.1.3、运行etcd\n`sudo systemctl start etcd`\n\n#### 2.1.4、配置etcd中的网络\n`etcdctl mk /k8s/network/config '{\"Network\":\"172.17.0.0/16\"}'`\n#### 2.2、kubernetes环境配置\n#### 2.2.1、复制命令（可执行文件）\n将位于\\_output/release-stage/server/linux-amd64/kubernetes/server/bin/目录下的kube-apiserver、kube-controller-manager、kube-scheduler、kubectl复制到Master节点的/usr/bin/目录下\n\n#### 2.2.2、创建相应的service文件以及配置文件（shell脚本）\n根据自己的的配置修改MASTER_ADDRESS和ETCD_SERVERS\n```\n#!/bin/bash\n# Copyright 2016 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nMASTER_ADDRESS=${1:-\"192.168.3.87\"}\nETCD_SERVERS=${2:-\"http://192.168.3.87:2379\"}\nSERVICE_CLUSTER_IP_RANGE=${3:-\"10.254.0.0/16\"}\nADMISSION_CONTROL=${4:-\"NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota\"}\n\ncat <<EOF >/etc/kubernetes/config\n# --logtostderr=true: log to standard error instead of files\nKUBE_LOGTOSTDERR=\"--logtostderr=true\"\n\n# --v=0: log level for V logs\nKUBE_LOG_LEVEL=\"--v=0\"\n\n# --allow-privileged=false: If true, allow privileged containers.\nKUBE_ALLOW_PRIV=\"--allow-privileged=false\"\n\n# How the controller-manager, scheduler, and proxy find the apiserver\nKUBE_MASTER=\"--master=${MASTER_ADDRESS}:8080\"\nEOF\n\ncat <<EOF >/etc/kubernetes/apiserver\n# --insecure-bind-address=127.0.0.1: The IP address on which to serve the --insecure-port.\nKUBE_API_ADDRESS=\"--insecure-bind-address=0.0.0.0\"\n\n# --insecure-port=8080: The port on which to serve unsecured, unauthenticated access.\nKUBE_API_PORT=\"--insecure-port=8080\"\n\n# --kubelet-port=10250: Kubelet port\nNODE_PORT=\"--kubelet-port=10250\"\n\n# --etcd-servers=[]: List of etcd servers to watch (http://ip:port),\n# comma separated. Mutually exclusive with -etcd-config\nKUBE_ETCD_SERVERS=\"--etcd-servers=${ETCD_SERVERS}\"\n\n# --advertise-address=<nil>: The IP address on which to advertise\n# the apiserver to members of the cluster.\nKUBE_ADVERTISE_ADDR=\"--advertise-address=${MASTER_ADDRESS}\"\n\n# --service-cluster-ip-range=<nil>: A CIDR notation IP range from which to assign service cluster IPs.\n# This must not overlap with any IP ranges assigned to nodes for pods.\nKUBE_SERVICE_ADDRESSES=\"--service-cluster-ip-range=${SERVICE_CLUSTER_IP_RANGE}\"\n\n# --admission-control=\"AlwaysAdmit\": Ordered list of plug-ins\n# to do admission control of resources into cluster.\n# Comma-delimited list of:\n#   LimitRanger, AlwaysDeny, SecurityContextDeny, NamespaceExists,\n#   NamespaceLifecycle, NamespaceAutoProvision,\n#   AlwaysAdmit, ServiceAccount, ResourceQuota, DefaultStorageClass\nKUBE_ADMISSION_CONTROL=\"--admission-control=${ADMISSION_CONTROL}\"\n\n# Add your own!\nKUBE_API_ARGS=\"\"\nEOF\n\nKUBE_APISERVER_OPTS=\"   \\${KUBE_LOGTOSTDERR}         \\\\\n                        \\${KUBE_LOG_LEVEL}           \\\\\n                        \\${KUBE_ETCD_SERVERS}        \\\\\n                        \\${KUBE_API_ADDRESS}         \\\\\n                        \\${KUBE_API_PORT}            \\\\\n                        \\${NODE_PORT}                \\\\\n                        \\${KUBE_ADVERTISE_ADDR}      \\\\\n                        \\${KUBE_ALLOW_PRIV}          \\\\\n                        \\${KUBE_SERVICE_ADDRESSES}   \\\\\n                        \\${KUBE_ADMISSION_CONTROL}   \\\\\n\t\t\t\t\t\t\\${KUBE_API_ARGS}\"\n\n\ncat <<EOF >/usr/lib/systemd/system/kube-apiserver.service\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=network.target\nAfter=etcd.service\n\n[Service]\nEnvironmentFile=-/etc/kubernetes/config\nEnvironmentFile=-/etc/kubernetes/apiserver\nExecStart=/usr/bin/kube-apiserver ${KUBE_APISERVER_OPTS}\nRestart=on-failure\nType=notify\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\ncat <<EOF >/etc/kubernetes/controller-manager\n###\n# The following values are used to configure the kubernetes controller-manager\n\n# defaults from config and apiserver should be adequate\n\n# Add your own!\nKUBE_CONTROLLER_MANAGER_ARGS=\"\"\nEOF\n\nKUBE_CONTROLLER_MANAGER_OPTS=\"  \\${KUBE_LOGTOSTDERR} \\\\\n                                \\${KUBE_LOG_LEVEL}   \\\\\n                                \\${KUBE_MASTER}      \\\\\n                                \\${KUBE_CONTROLLER_MANAGER_ARGS}\"\n\ncat <<EOF >/usr/lib/systemd/system/kube-controller-manager.service\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nEnvironmentFile=-/etc/kubernetes/config\nEnvironmentFile=-/etc/kubernetes/controller-manager\nExecStart=/usr/bin/kube-controller-manager ${KUBE_CONTROLLER_MANAGER_OPTS}\nRestart=on-failure\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\ncat <<EOF >/etc/kubernetes/scheduler\n###\n# kubernetes scheduler config\n\n# Add your own!\nKUBE_SCHEDULER_ARGS=\"\"\nEOF\n\nKUBE_SCHEDULER_OPTS=\"   \\${KUBE_LOGTOSTDERR}     \\\\\n                        \\${KUBE_LOG_LEVEL}       \\\\\n                        \\${KUBE_MASTER}          \\\\\n                        \\${KUBE_SCHEDULER_ARGS}\"\n\ncat <<EOF >/usr/lib/systemd/system/kube-scheduler.service\n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nEnvironmentFile=-/etc/kubernetes/config\nEnvironmentFile=-/etc/kubernetes/scheduler\nExecStart=/usr/bin/kube-scheduler ${KUBE_SCHEDULER_OPTS}\nRestart=on-failure\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsystemctl daemon-reload\n```\n\n\n#### 2.2.3、运行相应的Kubernetes命令（shell 脚本）\n\n```\nfor svc in etcd kube-apiserver kube-controller-manager kube-scheduler; do\n\tsystemctl restart $svc\n\tsystemctl enable $svc\n\tsystemctl status $svc\ndone\n```\n\n### 三、Node配置工作\n#### 3.1、安装flannel并修改配置文件\n#### 3.1.1、安装必要软件flannel\n`yum -y install flannel`\n#### 3.1.2、  修改flannel的配置文件/etc/sysconfig/flanneld\n```\nFLANNEL_ETCD=\"http://192.168.3.87:2379\"\nFLANNEL_ETCD_KEY=\"/k8s/network\"\n```\n#### 3.1.3、运行flannel\n\n`systemctl restart flanneld`\n`systemctl enable flanneld`\n`systemctl status flanneld`\n\n#### 3.1.4、上传网络配置\n创建一个config.json文件，内容如下：\n\n``` json?linenums\n{\n\"Network\": \"172.17.0.0/16\",\n\"SubnetLen\": 24,\n\"Backend\": {\n     \"Type\": \"vxlan\",\n     \"VNI\": 7890\n     }\n }\n```\n然后将配置上传到etcd服务器上：\n`curl -L http://192.168.3.87:2379/v2/keys/k8s/network/config -XPUT --data-urlencode value@config.json`\n\n#### 3.2、kubernetes环境配置\n#### 3.2.1、复制命令（可执行文件）\n将位于\\_output/release-stage/server/linux-amd64/kubernetes/server/bin/目录下的kube-proxy、kubelet 复制到Node节点的/usr/bin/目录下\n\n#### 3.2.2、创建相应的service文件以及配置文件（shell脚本）\n根据自己的的配置修改MASTER_ADDRESS和NODE_HOSTNAME\n``` bash?linenums\n#!/bin/bash\n# Copyright 2016 The Kubernetes Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nMASTER_ADDRESS=${1:-\"192.168.3.87\"}\nNODE_HOSTNAME=${2:-\"k8s-nod\"}\n\ncat <<EOF >/etc/kubernetes/config\n# --logtostderr=true: log to standard error instead of files\nKUBE_LOGTOSTDERR=\"--logtostderr=true\"\n\n# --v=0: log level for V logs\nKUBE_LOG_LEVEL=\"--v=0\"\n\n# --allow-privileged=false: If true, allow privileged containers.\nKUBE_ALLOW_PRIV=\"--allow-privileged=false\"\n\n# How the controller-manager, scheduler, and proxy find the apiserver\nKUBE_MASTER=\"--master=${MASTER_ADDRESS}:8080\"\nEOF\n\ncat <<EOF >/etc/kubernetes/proxy\n###\n# kubernetes proxy config\n\n# default config should be adequate\n\n# Add your own!\nKUBE_PROXY_ARGS=\"\"\nEOF\n\nKUBE_PROXY_OPTS=\"   \\${KUBE_LOGTOSTDERR} \\\\\n                    \\${KUBE_LOG_LEVEL}   \\\\\n                    \\${KUBE_MASTER}    \\\\\n                    \\${KUBE_PROXY_ARGS}\"\n\ncat <<EOF >/usr/lib/systemd/system/kube-proxy.service\n[Unit]\nDescription=Kubernetes Proxy\nAfter=network.target\n\n[Service]\nEnvironmentFile=-/etc/kubernetes/config\nEnvironmentFile=-/etc/kubernetes/kube-proxy\nExecStart=/usr/bin/kube-proxy ${KUBE_PROXY_OPTS}\nRestart=on-failure\nLimitNOFILE=65536\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\ncat <<EOF >/etc/kubernetes/kubelet\n# --address=0.0.0.0: The IP address for the Kubelet to serve on (set to 0.0.0.0 for all interfaces)\nKUBELET__ADDRESS=\"--address=0.0.0.0\"\n\n# --port=10250: The port for the Kubelet to serve on. Note that \"kubectl logs\" will not work if you set this flag.\nKUBELET_PORT=\"--port=10250\"\n\n# --hostname-override=\"\": If non-empty, will use this string as identification instead of the actual hostname.\nKUBELET_HOSTNAME=\"--hostname-override=${NODE_HOSTNAME}\"\n\n# --api-servers=[]: List of Kubernetes API servers for publishing events,\n# and reading pods and services. (ip:port), comma separated.\nKUBELET_API_SERVER=\"--api-servers=${MASTER_ADDRESS}:8080\"\n\n# pod infrastructure container\nKUBELET_POD_INFRA_CONTAINER=\"--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest\"\n\n# Add your own!\nKUBELET_ARGS=\"\"\nEOF\n\nKUBE_PROXY_OPTS=\"   \\${KUBE_LOGTOSTDERR}     \\\\\n                    \\${KUBE_LOG_LEVEL}       \\\\\n                    \\${KUBELET__ADDRESS}         \\\\\n                    \\${KUBELET_PORT}            \\\\\n                    \\${KUBELET_HOSTNAME}        \\\\\n                    \\${KUBELET_API_SERVER}   \\\\\n                    \\${KUBE_ALLOW_PRIV}      \\\\\n\t\t\t\t\t\\${KUBELET_POD_INFRA_CONTAINER}\\\\\n                    \\${KUBELET_ARGS}\"\n\ncat <<EOF >/usr/lib/systemd/system/kubelet.service\n[Unit]\nDescription=Kubernetes Kubelet\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nWorkingDirectory=/var/lib/kubelet\nEnvironmentFile=-/etc/kubernetes/config\nEnvironmentFile=-/etc/kubernetes/kubelet\nExecStart=/usr/bin/kubelet ${KUBE_PROXY_OPTS}\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsystemctl daemon-reload\n```\n\n#### 3.2.3、运行相应的Kubernetes命令（shell脚本）\n```bash?linenums\nfor svc in kube-proxy kubelet docker; do\n\tsystemctl restart $svc\n\tsystemctl enable $svc\n\tsystemctl status $svc\ndone\n```\n### 四、验证配置以及创建dashboard\n#### 4.1、验证环境配置\n在Master节点运行命令`kubectl get nodes`，输出信息如下：\n\n``` markdown\n[root@k8s-mst ~]# kubectl get nodes\n NAME      STATUS    AGE\n nod1       Ready     3h\n nod2       Ready     3h\n```\n#### 4.2、搭建dashboard\n#### 4.2.1、创建命名空间（namespace）kube-system\n创建文件kubernetes-namespace.jason，内容如下：\n\n```\n{\n  \"kind\": \"Namespace\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"kube-system\"\n  }\n}\n```\n使用命令`kubectl create -f kubernetes-namespace.jason`创建kube-system命名空间。\n#### 4.2.2、创建dashboard\n创建kubernetes-dashboard.yaml文件\n```\n# Configuration to deploy release version of the Dashboard UI.\n# Example usage: kubectl create -f <this_file>\n\nkind: Deployment\napiVersion: extensions/v1beta1\nmetadata:\n  labels:\n    app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubernetes-dashboard\n  template:\n    metadata:\n      labels:\n        app: kubernetes-dashboard\n      # Comment the following annotaion if Dashboard must not be deployed on master\n      annotations:\n        scheduler.alpha.kubernetes.io/tolerations: |\n          [\n            {\n              \"key\": \"dedicated\",\n              \"operator\": \"Equal\",\n              \"value\": \"master\",\n              \"effect\": \"NoSchedule\"\n            }\n          ]\n    spec:\n      containers:\n      - name: kubernetes-dashboard\n        image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.2\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        args:\n          # Uncomment the following line to manually specify Kubernetes API server Host\n          # If not specified, Dashboard will attempt to auto discover the API server and connect\n          # to it. Uncomment only if the default does not work.\n          - --apiserver-host=http://192.168.3.87:8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 9090\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n---\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kube-system\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: 9090\n  selector:\n    app: kubernetes-dashboard\n```\n\n注意：\n>  1. 修改 --apiserver-host=http://***192.168.3.87***:8080；\n>  2. 其中 “ image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.2 ”，使用的是谷歌镜像库，需要Node节点翻墙才会可能正常创建。\n>\n\n- 使用命令`kubectl create -f kubernetes-dashboard.yaml`创建kubernetes-dashboard Deployment和Service。\n\n- 如无法获取相应的镜像创建之后，使用命令`kubectl get pod  --namespace=\"kube-system\"`查看会显示如下结果：\n\n``` mel\n[root@mst ~]# kubectl get pod --namespace=\"kube-system\"\nNAME                                  READY     STATUS         RESTARTS   AGE\nkubernetes-dashboard-47291540-lcuox   0/1       ErrImagePull   0          1m\n```\n\n- 搭建自己的镜像库，可参考【Docker实战】Registry & Portus搭建详解 - 龍隐 - 博客园  http://www.cnblogs.com/xcloudbiz/articles/5497037.html\n\n#### 4.2.3、使用dashboard\n成功运行之后：\n\n```\n[root@k8s-mst ~]# kubectl get pod --namespace=\"kube-system\"\nNAME                                    READY     STATUS    RESTARTS   AGE\nkubernetes-dashboard-3856900779-226mr   1/1       Running   0          2m\n\n[root@k8s-mst ~]# kubectl describe  pod kubernetes-dashboard-3856900779-226mr --namespace=\"kube-system\"\nName:\t\tkubernetes-dashboard-3856900779-226mr\nNamespace:\tkube-system\nNode:\t\tnod1/192.168.3.91\nStart Time:\tTue, 22 Nov 2016 20:33:04 +0800\nLabels:\t\tapp=kubernetes-dashboard\n\t\tpod-template-hash=3856900779\nStatus:\t\tRunning\nIP:\t\t172.18.0.2\nControllers:\tReplicaSet/kubernetes-dashboard-3856900779\nContainers:\n  kubernetes-dashboard:\n    Container ID:\tdocker://d36cff522129c73f0de370857124697659662c99d370af548a1367604bac7014\n    Image:\t\tgcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.2\n    Image ID:\t\tdocker://sha256:c0e4ba8968ee756368cbe5f64f39b0ef8e128de90d0bdfe1d040f0773055e68a\n    Port:\t\t9090/TCP\n    Args:\n      --apiserver-host=http://192.168.3.87:8080\n    State:\t\t\tRunning\n      Started:\t\t\tTue, 22 Nov 2016 20:35:00 +0800\n    Ready:\t\t\tTrue\n    Restart Count:\t\t0\n    Liveness:\t\t\thttp-get http://:9090/ delay=30s timeout=30s period=10s #success=1 #failure=3\n    Volume Mounts:\t\t<none>\n    Environment Variables:\t<none>\nConditions:\n  Type\t\tStatus\n  Initialized \tTrue\n  Ready \tTrue\n  PodScheduled \tTrue\nNo volumes.\nQoS Class:\tBestEffort\nTolerations:\tdedicated=master:Equal:NoSchedule\nEvents:\n  FirstSeen\tLastSeen\tCount\tFrom\t\t\tSubobjectPath\t\t\t\tType\t\tReason\t\t\tMessage\n  ---------\t--------\t-----\t----\t\t\t-------------\t\t\t\t--------\t------\t\t\t-------\n  4m\t\t4m\t\t1\t{default-scheduler }\t\t\t\t\t\tNormal\t\tScheduled\t\tSuccessfully assigned kubernetes-dashboard-3856900779-226mr to nod1\n  3m\t\t3m\t\t1\t{kubelet nod1}\t\tspec.containers{kubernetes-dashboard}\tNormal\t\tPulling\t\t\tpulling image \"gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.2\"\n  3m\t\t2m\t\t2\t{kubelet nod1}\t\t\t\t\t\t\tWarning\t\tMissingClusterDNS\tkubelet does not have ClusterDNS IP configured and cannot create Pod using \"ClusterFirst\" policy. Falling back to DNSDefault policy.\n  2m\t\t2m\t\t1\t{kubelet nod1}\t\tspec.containers{kubernetes-dashboard}\tNormal\t\tPulled\t\t\tSuccessfully pulled image \"gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.2\"\n  2m\t\t2m\t\t1\t{kubelet nod1}\t\tspec.containers{kubernetes-dashboard}\tNormal\t\tCreated\t\t\tCreated container with docker id d36cff522129; Security:[seccomp=unconfined]\n  2m\t\t2m\t\t1\t{kubelet nod1}\t\tspec.containers{kubernetes-dashboard}\tNormal\t\tStarted\t\t\tStarted container with docker id d36cff522129\n\n[root@mst ~]# kubectl describe service  kubernetes-dashboard  --namespace=\"kube-system\"\nName:\t\t\tkubernetes-dashboard\nNamespace:\t\tkube-system\nLabels:\t\t\tapp=kubernetes-dashboard\nSelector:\t\tapp=kubernetes-dashboard\nType:\t\t\tNodePort\nIP:\t\t\t10.254.196.154\nExternal IPs:\t\t192.168.3.87\nPort:\t\t\t<unset>\t80/TCP\nNodePort:\t\t<unset>\t31437/TCP\nEndpoints:\t\t172.18.0.2:9090\nSession Affinity:\tNone\n```\n\n运行成功之后，就可以使用浏览器访问192.168.3.89:31437使用dashboard。\n\n\n\n\n","tags":["Kubernetes","Docker"],"categories":["搞机记录"]},{"title":"CRIU的安装与使用","url":"/20161108/CRIU的安装与使用/","content":"\n\n### CRIU安装：\n##### 获取CRIU的源代码\ngit clonehttps://github.com/xemul/criu\n或者\nwget http://download.openvz.org/criu/criu-x.x.tar.bz2\ntar -xvf criu-x.x.tar.bz2\n##### 安装编译依赖软件\nyum install gcc make -y\nyum install glibc-devel.i686 protobuf protobuf-c protobuf-c-devel protobuf-compiler protobuf-devel protobuf-python libaio-devel libcap-devel libnl3-devel -y\n在CRIU代码根目录下，执行编译命令 make\n##### 安装依赖软件\nyum install asciidoc xmlto -y\n在CRIU代码根目录下，运行安装命令 make install\n\n### CRIU使用：\n命令格式：criu dump -D chkpoint -t pid\nCRIU环境检查, 创建一个无限循环脚本文件。\n\n![](CRIU的安装与使用/20161108163852230.png)\n\n并运行脚本文件。\n\n[root@de69 ~]# chmod +x test.sh\n\n[root@de69 ~]# ./test.sh\n\n创建一个新的终端，获得test.sh的pid，“pgrep -f test.sh”\n\n[root@de69 ~]# criu dump -t $PID --images-dir chkpoint --shell-job\n\n![](CRIU的安装与使用/20161108163926605.png)\n\n[root@de69 ~]# criu restore -t 23267 --images-dir chkpoint/ --shell-job\n 运行命令之后test.sh恢复正常运行。\n\n\n#### 在使用CRIU对Docker容器进行操作时出现以下提示错误\n> Error (criu/namespaces.c:403): Can't dump nested pid namespace for 23025\n>\n> Error (criu/namespaces.c:607): Can't make pidns id\n>\n> Error (criu/cr-dump.c:1625): Dumping FAILED.\n\n![](CRIU的安装与使用/20161108163956808.png)\n\n参看pid=23025的应用\n\n![](CRIU的安装与使用/20161108164007875.png)\n\n[root@de69 ~]# criu dump -t 23025 --images-dir /tmp/doc01/\n![](CRIU的安装与使用/20161108164012230.png)\n\n原因是当前CRIU并不完善（如不支持seccomp、不支持外部终端、挂载的文件系统可读等），Docker容器对宿主机的应用（例如“/bin/bash”等）有依赖，而CRIU不能对非Docker容器进程树中的进程设置检查点，从而导致Checkpoint/Restore失败。\n","tags":["CRIU"],"categories":["搞机记录"]},{"title":"KVM虚拟机在线迁移环境配置","url":"/20160611/KVM虚拟机在线迁移环境配置/","content":"\n简要配置步骤：\n-------\n 1. 在两台服务器上安装CentOS7的系统。\n 2. 编译使用4.3.0版本内核，注意配置KVM以及虚拟设备驱动等模块。\n 3. 实现两台主机无密码ssh登录，\n     > 1)运行：ssh-keygen -t rsa ;\n     > 2)然后拍两下回车（均选择默认） ;\n     > 3)运行：ssh-copy-id -i /root/.ssh/id_rsa.pub root@target_host;\n     > 4)再输入163机器上的root密码。\n\n 4. 关闭源与目标主机的防火墙（否则出现错误“Unable to migrate guest: unable to connect to\n    server at 'target_addr:49152': No route to host”）。\n 5. 确保两台主机libvirtd都在运行。\n 6. 源主机端打开virt-manager，Add Connection，添加对目标主机的链接。\n 7. 在源主机创建虚拟机，并在目标主机相同的路径创建大于等于源虚拟机大小，相同格式相同名称的镜像文件（否则出现error：Unable to\n    migrate guest: Cannot access storage file '/home/images/vm01.qcow2'\n    (as uid:107, gid:107): No such file or directory）。\n 8. 可以使用virt-manage进行libvirtd、tcp在线迁移，如命令：virsh migrate --live --copy-storage-all vmname qemu+ssh(or tcp)://dest_ip/system\n\n可能出现的错误或者问题及其解决方法：\n------------------\n\n执行迁移命令的时候\n1、出现错误：“error: internal error: unable to execute QEMU command 'migrate': this feature or command is not currently supported”。\n\n通过查询(Re: [libvirt-users] Libvirt Live Migration  https://www.redhat.com/archives/libvirt-users/2014-December/msg00008.html)说是qemu-kvm版本的问题，使用的是qemu-kvm-1.5.3。网上提出问题的人说他使用qemu-kvm-1.0的可以。解决方法：推荐使用qemu-kvm-rhev 代替 qemu-kvm。\n\n2、 出现错误：“Unable to migrate guest: internal error: process exited while connecting to monitor: Could not access KVM kernel module: Permission denied failed to initialize KVM: Permission denied“\n\n解决方法：修改/etc/libvirt/qemu.conf，使user=root，group=root，dynamic_ownership = 0然后重启服务libvirtd。\n\n3、出现错误：“Could not access KVM kernel module: Permission denied failed to initialize KVM :  Permission denied\n\n解决方法：rmmod kvm_intel; rmmod kvm; modprobe kvm; modprobe kvm_intel\n\n4、出现错误：“Error starting domain: internal error: process exited while connecting to monitor: qemu-system-x86_64:  …… Can't use 'raw' as a block driver for the protocol level”\n\n解决方法： 删除CD-ROM模块。\n","tags":["KVM","Live Migration"],"categories":["搞机记录"]}]